

### Decrease and Conquer Overview:
- **Decrease and Conquer** is a problem-solving strategy that involves reducing the problem's size with each step. The solution to a smaller instance is extended to solve the larger problem.
- The approach can be implemented **recursively** (top-down) or **iteratively** (bottom-up).

### Three Variants of Decrease and Conquer:
1. **Decrease by a Constant**:
   - The problem size is reduced by a fixed amount at each step (e.g., 1).
   - Examples: **Insertion Sort** and **Topological Sort**.
   
2. **Decrease by a Constant Factor**:
   - The problem size is reduced by a constant factor (e.g., halved).
   - Example: **Fake Coin Problem**.

3. **Decrease by a Variable Size**:
   - The problem size is reduced by a variable amount at each step.
   - Example: **Greatest Common Divisor (GCD) Algorithm**.

---

### Key Examples:

1. **Decrease by a Constant: Insertion Sort**:
   - Assume the list of size \(n-1\) is sorted, and the \(n\)th element is inserted into the correct position.
   - The recursive version works, but the **iterative** method is often preferred for efficiency.
   - Worst-case efficiency is \( \Theta(n^2) \), but it performs well on partially sorted data.
   - [Source for Insertion Sort Example, Lecture 6+7c, page 1](6)

2. **Decrease by a Constant: Topological Sort**:
   - In a **Directed Acyclic Graph (DAG)**, vertices are sorted so that for every directed edge \( u \to v \), \( u \) appears before \( v \) in the ordering.
   - This can be solved with **Depth First Search (DFS)**.
   - Application examples include scheduling tasks in a project or ordering courses based on prerequisites.
   - [Source for Topological Sort Example, Lecture 6+7c, page 7](6)

3. **Decrease by a Constant Factor: Fake Coin Problem**:
   - In this problem, among \( n \) coins, one is fake and weighs less.
   - The strategy involves dividing the coins into two equal piles (or almost equal, leaving one coin out if the number is odd).
   - Weigh the two piles. If one is lighter, it contains the fake coin; otherwise, the extra coin is fake.
   - This approach runs in \( \Theta(\log n) \) time.
   - There is an alternative version where coins are divided into three piles, improving efficiency by a factor of 1.6 (reducing time to \( \Theta(\log_3 n) \)).
   - [Source for Fake Coin Problem Example, Lecture 5c, pages 28-30](7)

4. **Generating Permutations (Decrease by 1)**:
   - Permutations are generated by considering permutations of a smaller list and inserting the \( n \)-th element in every possible position.
   - **Steinhaus-Johnson-Trotter Algorithm** is an alternative, where permutations are generated without creating smaller sub-problems by keeping track of the next permutation using arrows.
   - [Source for Permutations Example, Lecture 6+7c, page 10](6)

5. **Greatest Common Divisor (GCD) Algorithm (Variable Size Decrease)**:
   - Finds the GCD of two integers using the relation \( gcd(a, b) = gcd(b, a \mod b) \).
   - The size of the problem decreases variably based on the remainder from the division.
   - Example: \( gcd(60, 24) = gcd(24, 12) = gcd(12, 0) = 12 \).
   - [Source for GCD Example, Lecture 6+7c, page 26](6)

---

Here are the remaining notes on "decrease and conquer" strategies based on your PDFs:

### Additional Decrease by a Constant Factor Examples:

6. **Multiplication à la Russe**:
   - Also known as Russian Peasant Multiplication, this is a method of multiplying two numbers by halving one number and doubling the other.
   - For instance, to multiply 50 by 20:
     - \( 50 \times 20 = 25 \times 40 = 12 \times 80 + 40 = 6 \times 160 + 40 = 3 \times 320 + 40 = 1 \times 640 + 320 + 40 = 1000 \).
   - This method decreases one number by a factor of 2 at each step and is efficient in hardware (shift operations instead of multiplication).
   - The time complexity is \( \Theta(\log n) \) due to the halving process.
   - [Source for Multiplication à la Russe Example, Lecture 6+7c, page 24](6)

7. **Interpolation Search** (for sorted arrays):
   - This search method improves on binary search by guessing where the desired value might be based on its position in a sorted array.
   - It assumes that the array values increase linearly between the leftmost and rightmost elements.
   - The idea is to calculate the approximate position of the search element, which often makes fewer comparisons than binary search.
   - The worst-case time complexity is \( O(n) \), but the average case is better.
   - [Source for Interpolation Search Example, Lecture 6+7c, page 32](6)

---

### Decrease by a Variable Size Examples:

8. **Finding the k-th Order Statistic (Quickselect Algorithm)**:
   - The problem is to find the k-th smallest element in an unordered list. This can be done using a partitioning scheme similar to QuickSort.
   - By repeatedly partitioning the list around a pivot, the algorithm gradually narrows down to the k-th smallest element.
   - **Quickselect** has an average time complexity of \( \Theta(n) \), but in the worst case, it is \( \Theta(n^2) \).
   - [Source for Quickselect Example, Lecture 6+7c, pages 27-31](6)

9. **Efficient Matrix Multiplication (Strassen’s Method)**:
   - The brute-force multiplication of two \( n \times n \) matrices takes \( \Theta(n^3) \) operations.
   - **Strassen’s Algorithm** reduces this to approximately \( \Theta(n^{2.81}) \) by recursively breaking the matrices into smaller submatrices and reducing the number of multiplication operations.
   - This is achieved by trading off multiplication for addition, which is cheaper in terms of computational cost.
   - [Source for Strassen’s Matrix Multiplication Example, Lecture 6+7c, page 45](6)

---

### General Efficiency and Applications of Decrease and Conquer:

- **Strengths**:
  - Decrease and conquer strategies can be implemented **recursively** (top-down) or **iteratively** (bottom-up).
  - Efficient for certain problems, leading to logarithmic or linear time complexities in many cases (e.g., \( \Theta(\log n) \)).
  - Useful for tasks like **graph traversal** (both Breadth-First and Depth-First Search rely on this technique).
  
- **Weaknesses**:
  - Less widely applicable, especially for the **constant factor** reduction method.
  - Some cases, like **variable size decrease** problems, can still result in inefficient algorithms (e.g., \( \Theta(n^2) \) in the worst case for Quickselect).

---



---

### 1. **Insertion Sort (Decrease by a Constant)**

```python
def insertion_sort(arr):
    for i in range(1, len(arr)):
        key = arr[i]
        j = i - 1
        # Move elements greater than key to one position ahead
        while j >= 0 and arr[j] > key:
            arr[j + 1] = arr[j]
            j -= 1
        arr[j + 1] = key

# Example usage:
arr = [12, 11, 13, 5, 6]
insertion_sort(arr)
print("Sorted array:", arr)
```

**Explanation:** The algorithm builds the sorted array one element at a time by inserting each new element into its proper place among the previously sorted elements.

---

### 2. **Topological Sort (Decrease by a Constant)**

```python
from collections import defaultdict

def topological_sort_util(v, visited, stack, graph):
    visited[v] = True
    for i in graph[v]:
        if not visited[i]:
            topological_sort_util(i, visited, stack, graph)
    stack.insert(0, v)  # Equivalent to push operation

def topological_sort(graph, vertices):
    visited = {v: False for v in vertices}
    stack = []
    for v in vertices:
        if not visited[v]:
            topological_sort_util(v, visited, stack, graph)
    return stack

# Example usage:
graph = defaultdict(list)
# Adding edges to the graph
graph['5'].extend(['0', '2'])
graph['4'].extend(['0', '1'])
graph['2'].append('3')
graph['3'].append('1')

vertices = ['0', '1', '2', '3', '4', '5']
order = topological_sort(graph, vertices)
print("Topological Sort Order:", order)
```

**Explanation:** This uses DFS to perform a topological sort on a DAG, ordering tasks based on dependencies.

---

### 3. **Fake Coin Problem (Decrease by a Constant Factor)**

```python
def find_fake_coin(coins, offset=0):
    n = len(coins)
    if n == 1:
        return offset
    elif n == 2:
        return offset if coins[0] < coins[1] else offset + 1
    else:
        mid = n // 2
        left = coins[:mid]
        right = coins[mid:mid*2]
        left_weight = sum(left)
        right_weight = sum(right)
        if left_weight < right_weight:
            return find_fake_coin(left, offset)
        elif left_weight > right_weight:
            return find_fake_coin(right, offset + mid)
        else:
            if n > mid*2:
                return offset + mid*2  # The extra coin is fake
            else:
                return -1  # No fake coin found

# Example usage:
coins = [10, 10, 10, 10, 10, 10, 9]  # The last coin is fake (lighter)
index = find_fake_coin(coins)
print(f"The fake coin is at index {index}")
```

**Explanation:** This function recursively divides the coins into halves to find the lighter fake coin efficiently.

---

### 4. **Generating Permutations (Decrease by 1)**

```python
def permutations(lst):
    if len(lst) == 0:
        return []
    elif len(lst) == 1:
        return [lst]
    else:
        perms = []
        for i in range(len(lst)):
            m = lst[i]
            rem_lst = lst[:i] + lst[i+1:]
            for p in permutations(rem_lst):
                perms.append([m] + p)
        return perms

# Example usage:
lst = [1, 2, 3]
perm_list = permutations(lst)
print("Permutations:")
for perm in perm_list:
    print(perm)
```

**Explanation:** The function recursively generates permutations by fixing each element and permuting the remaining list.

---

### 5. **Greatest Common Divisor (Variable Size Decrease)**

```python
def gcd(a, b):
    if b == 0:
        return a
    else:
        return gcd(b, a % b)

# Example usage:
result = gcd(60, 24)
print("GCD is:", result)
```

**Explanation:** This is the classic Euclidean algorithm, which reduces the problem size by the remainder.

---

### 6. **Multiplication à la Russe**

```python
def russian_peasant(a, b):
    result = 0
    while b > 0:
        if b % 2 != 0:
            result += a
        a <<= 1  # Double the number
        b >>= 1  # Halve the number
    return result

# Example usage:
product = russian_peasant(50, 20)
print("Product is:", product)
```

**Explanation:** This method multiplies two numbers by halving and doubling, adding the current value when the halved number is odd.

---

### 7. **Interpolation Search**

```python
def interpolation_search(arr, x):
    low = 0
    high = len(arr) - 1
    while low <= high and arr[low] != arr[high]:
        # Estimate the position
        pos = low + ((x - arr[low]) * (high - low)) // (arr[high] - arr[low])
        if pos < 0 or pos >= len(arr):
            return -1
        if arr[pos] == x:
            return pos
        elif arr[pos] < x:
            low = pos + 1
        else:
            high = pos - 1
    if arr[low] == x:
        return low
    return -1

# Example usage:
arr = [10, 12, 13, 16, 18, 19, 20, 21, 22, 23, 24, 33, 35, 42, 47]
x = 18
index = interpolation_search(arr, x)
print(f"Element found at index {index}")
```

**Explanation:** Interpolation search improves upon binary search by estimating the position of the target value.

---

### 8. **Quickselect Algorithm (Finding the k-th Order Statistic)**

```python
def quickselect(arr, k):
    if len(arr) == 1:
        return arr[0]
    pivot = arr[len(arr) // 2]
    lows = [el for el in arr if el < pivot]
    highs = [el for el in arr if el > pivot]
    pivots = [el for el in arr if el == pivot]
    if k < len(lows):
        return quickselect(lows, k)
    elif k < len(lows) + len(pivots):
        return pivots[0]
    else:
        return quickselect(highs, k - len(lows) - len(pivots))

# Example usage:
arr = [12, 3, 5, 7, 4, 19, 26]
k = 3  # Looking for the 4th smallest element (0-indexed)
result = quickselect(arr, k)
print(f"The {k+1}-th smallest element is {result}")
```

**Explanation:** Quickselect partitions the array around a pivot to find the k-th smallest element efficiently.

---

### 9. **Strassen’s Matrix Multiplication**

```python
import numpy as np

def strassen(A, B):
    assert A.shape == B.shape
    n = A.shape[0]
    if n == 1:
        return A * B
    else:
        # Divide matrices into quadrants
        mid = n // 2
        A11 = A[:mid, :mid]
        A12 = A[:mid, mid:]
        A21 = A[mid:, :mid]
        A22 = A[mid:, mid:]

        B11 = B[:mid, :mid]
        B12 = B[:mid, mid:]
        B21 = B[mid:, :mid]
        B22 = B[mid:, mid:]

        # Compute the 7 products
        M1 = strassen(A11 + A22, B11 + B22)
        M2 = strassen(A21 + A22, B11)
        M3 = strassen(A11, B12 - B22)
        M4 = strassen(A22, B21 - B11)
        M5 = strassen(A11 + A12, B22)
        M6 = strassen(A21 - A11, B11 + B12)
        M7 = strassen(A12 - A22, B21 + B22)

        # Compute the result quadrants
        C11 = M1 + M4 - M5 + M7
        C12 = M3 + M5
        C21 = M2 + M4
        C22 = M1 - M2 + M3 + M6

        # Combine quadrants into a full matrix
        C = np.zeros((n, n), dtype=A.dtype)
        C[:mid, :mid] = C11
        C[:mid, mid:] = C12
        C[mid:, :mid] = C21
        C[mid:, mid:] = C22
        return C

# Example usage:
A = np.array([[1, 3], [7, 5]])
B = np.array([[6, 8], [4, 2]])
C = strassen(A, B)
print("Product matrix:\n", C)
```

**Explanation:** Strassen’s algorithm multiplies matrices faster than the standard method by reducing the number of recursive multiplications.

---










---




---
### Divide and Conquer Overview
- **Divide and Conquer** is a well-known algorithmic design technique that works by:
  1. **Dividing** the problem into two or more smaller subproblems of the same or related type.
  2. **Conquering** the subproblems by solving them recursively.
  3. **Combining** the solutions of the subproblems to get the solution to the original problem.

- **Recurrence Relation**:
  The recurrence relation for divide and conquer algorithms is usually of the form:
  \[
  T(n) = aT\left(\frac{n}{b}\right) + f(n)
  \]
  Where:
  - \(a\) is the number of subproblems,
  - \(n/b\) is the size of each subproblem,
  - \(f(n)\) is the cost of dividing the problem and combining the results.

- **Master Theorem**:
  The Master Theorem provides a way to analyze the time complexity of divide and conquer algorithms based on their recurrence relation. For example, when \( a = b^d \), the complexity is \( \Theta(n^d \log n) \).

---

### Key Examples of Divide and Conquer Algorithms

1. **Mergesort**:
   - **Algorithm**:
     - Split the input array into two halves.
     - Recursively sort both halves.
     - Merge the two sorted halves into a single sorted array.
   - **Complexity**:
     - Recurrence: \(T(n) = 2T(n/2) + O(n)\)
     - Time Complexity: \(O(n \log n)\)
     - Space Complexity: \(O(n)\)
     - Example: Sorting an array of size \(n\) by breaking it down into smaller arrays until the size reaches 1, then merging.
     - [Lecture 6+7c, page 36](23)

2. **Quicksort**:
   - **Algorithm**:
     - Select a pivot element.
     - Partition the array into two subarrays, with elements less than or equal to the pivot on one side and elements greater than the pivot on the other.
     - Recursively sort both subarrays.
   - **Complexity**:
     - Best/Average Case: \(O(n \log n)\)
     - Worst Case: \(O(n^2)\), occurs when the pivot is the smallest or largest element.
     - Improvement: Using the "median of three" partitioning reduces the chance of encountering the worst case.
     - Example: Sorting an array by selecting a pivot and recursively sorting partitions.
     - [Lecture 6+7c, pages 38-43](23)

3. **Multiplying Large Integers**:
   - **Problem**: Multiplying very large numbers that cannot fit into a computer word size.
   - **Strassen’s Algorithm**:
     - A divide and conquer algorithm that reduces the number of multiplications needed when multiplying matrices.
     - Breaks down two matrices into smaller matrices and combines the results.
   - **Complexity**:
     - Time complexity: \(O(n^{2.81})\), which is faster than the traditional \(O(n^3)\) matrix multiplication.
     - Example: Using Strassen’s method to multiply large matrices more efficiently.
     - [Lecture 6+7c, pages 44-45](23)

4. **Closest Pair of Points (Geometric Problem)**:
   - **Problem**: Given a set of points on a 2D plane, find the pair with the smallest Euclidean distance.
   - **Algorithm**:
     - Sort the points based on their x-coordinates.
     - Divide the set into two halves.
     - Recursively find the closest pair in each half.
     - Check for pairs straddling the boundary between the two halves.
   - **Complexity**:
     - Time complexity: \(O(n \log n)\), which is faster than the brute-force \(O(n^2)\).
     - Example: Applying the algorithm to find the closest pair of points on a plane.
     - [lec8.pdf, pages 23-26](22)

5. **Convex Hull Problem (Quickhull)**:
   - **Problem**: Find the convex hull (the smallest polygon enclosing all points) from a set of points on a 2D plane.
   - **Algorithm (Quickhull)**:
     - Similar to Quicksort, select the extreme points (e.g., leftmost and rightmost).
     - Recursively find points that form the upper and lower hulls.
   - **Complexity**:
     - Worst-case complexity: \(O(n^2)\).
     - Average-case complexity: \(O(n \log n)\).
     - Example: Using Quickhull to find the convex hull of a set of points.
     - [lec8.pdf, pages 30-31](22)

6. **Matrix Multiplication**:
   - **Strassen’s Method**: An efficient divide and conquer algorithm for matrix multiplication, reducing the number of multiplications.
   - **Algorithm**:
     - Divide matrices into smaller submatrices.
     - Apply recursive multiplication and combining steps.
   - **Complexity**:
     - Time complexity: \(O(n^{2.81})\).
     - [Lecture 6+7c, page 45](23)

---

### Divide and Conquer Efficiency
- Divide and conquer algorithms are often highly efficient for solving large problems by breaking them down into smaller, manageable subproblems.
- They are especially useful when:
  - **Direct problem-solving** is inefficient (e.g., sorting, searching, geometric problems).
  - **Recursion** can be applied to solve subproblems of the same nature.
- However, they may require **extra space** (e.g., Mergesort) and can have a **high recursion overhead** if not optimized for tail recursion.

---

---

### 1. **Mergesort**

```python
def merge_sort(arr):
    if len(arr) > 1:
        # Divide
        mid = len(arr) // 2
        left_half = arr[:mid]
        right_half = arr[mid:]

        # Conquer (Recursively sort both halves)
        merge_sort(left_half)
        merge_sort(right_half)

        # Combine
        i = j = k = 0

        # Copy data to temp arrays left_half[] and right_half[]
        while i < len(left_half) and j < len(right_half):
            if left_half[i] < right_half[j]:
                arr[k] = left_half[i]
                i += 1
            else:
                arr[k] = right_half[j]
                j += 1
            k += 1

        # Checking if any element was left
        while i < len(left_half):
            arr[k] = left_half[i]
            i += 1
            k += 1

        while j < len(right_half):
            arr[k] = right_half[j]
            j += 1
            k += 1

# Example usage:
arr = [38, 27, 43, 3, 9, 82, 10]
merge_sort(arr)
print("Sorted array:", arr)
```

**Explanation:**

- **Divide**: The array is recursively split into halves until the size becomes 1.
- **Conquer**: Each half is sorted recursively.
- **Combine**: The sorted halves are merged to produce sorted arrays.

---

### 2. **Quicksort**

```python
def quicksort(arr, low, high):
    if low < high:
        # Partitioning index
        pi = partition(arr, low, high)

        # Recursively sort elements before and after partition
        quicksort(arr, low, pi - 1)
        quicksort(arr, pi + 1, high)

def partition(arr, low, high):
    # Pivot (Here, we choose the last element as pivot)
    pivot = arr[high]
    i = low - 1  # Index of smaller element

    for j in range(low, high):
        # If current element is smaller than or equal to pivot
        if arr[j] <= pivot:
            i += 1  # Increment index of smaller element
            arr[i], arr[j] = arr[j], arr[i]  # Swap

    # Swap the pivot element with the element at i+1
    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1

# Example usage:
arr = [10, 7, 8, 9, 1, 5]
quicksort(arr, 0, len(arr) - 1)
print("Sorted array:", arr)
```

**Explanation:**

- **Divide**: The array is partitioned into subarrays around a pivot.
- **Conquer**: Recursively sort the subarrays before and after the pivot.
- **Combine**: The subarrays are inherently combined as the recursion unwinds.

---

### 3. **Multiplying Large Integers (Karatsuba Algorithm)**

```python
def karatsuba(x, y):
    # Base case for recursion
    if x < 10 or y < 10:
        return x * y
    else:
        # Calculates the size of the numbers
        n = max(len(str(x)), len(str(y)))
        m = n // 2

        # Split the digit sequences about the middle
        high_x, low_x = divmod(x, 10**m)
        high_y, low_y = divmod(y, 10**m)

        # 3 calls made to numbers approximately half the size
        z0 = karatsuba(low_x, low_y)
        z1 = karatsuba((low_x + high_x), (low_y + high_y))
        z2 = karatsuba(high_x, high_y)

        return (z2 * 10**(2 * m)) + ((z1 - z2 - z0) * 10**m) + z0

# Example usage:
x = 12345678
y = 87654321
product = karatsuba(x, y)
print(f"Product of {x} and {y} is {product}")
```

**Explanation:**

- **Divide**: The numbers are split into high and low parts.
- **Conquer**: Recursively compute three products of smaller numbers.
- **Combine**: Use the Karatsuba formula to combine the products.

---

### 4. **Closest Pair of Points**

```python
import math

def closest_pair_of_points(points):
    def distance(p1, p2):
        return math.hypot(p1[0] - p2[0], p1[1] - p2[1])

    def closest_pair_recursive(px, py):
        n = len(px)
        if n <= 3:
            # Brute-force approach
            min_dist = float('inf')
            pair = None
            for i in range(n):
                for j in range(i + 1, n):
                    d = distance(px[i], px[j])
                    if d < min_dist:
                        min_dist = d
                        pair = (px[i], px[j])
            return min_dist, pair

        # Divide
        mid = n // 2
        Qx = px[:mid]
        Rx = px[mid:]
        midpoint = px[mid][0]
        Qy = list()
        Ry = list()
        for point in py:
            if point[0] <= midpoint:
                Qy.append(point)
            else:
                Ry.append(point)

        # Conquer
        (dl, pair_left) = closest_pair_recursive(Qx, Qy)
        (dr, pair_right) = closest_pair_recursive(Rx, Ry)

        # Find smaller distance
        if dl < dr:
            d = dl
            min_pair = pair_left
        else:
            d = dr
            min_pair = pair_right

        # Combine
        strip = [p for p in py if abs(p[0] - midpoint) < d]
        min_d_strip, pair_strip = closest_strip_pair(strip, d, min_pair, distance)
        if min_d_strip < d:
            return min_d_strip, pair_strip
        else:
            return d, min_pair

    def closest_strip_pair(strip, d, min_pair, distance):
        min_d = d
        n = len(strip)
        for i in range(n):
            for j in range(i + 1, min(i + 7, n)):
                p, q = strip[i], strip[j]
                dst = distance(p, q)
                if dst < min_d:
                    min_d = dst
                    min_pair = (p, q)
        return min_d, min_pair

    # Preprocess: sort point lists
    px = sorted(points, key=lambda x: x[0])
    py = sorted(points, key=lambda x: x[1])

    # Run recursive function
    dist, pair = closest_pair_recursive(px, py)
    return dist, pair

# Example usage:
points = [(2, 3), (12, 30), (40, 50), (5, 1), (12, 10), (3, 4)]
distance, pair = closest_pair_of_points(points)
print(f"The closest pair is {pair} with a distance of {distance}")
```

**Explanation:**

- **Divide**: Split the set of points into two halves based on x-coordinates.
- **Conquer**: Recursively find the closest pair in each half.
- **Combine**: Check for any pair with one point in each half that may be closer.

---

### 5. **Convex Hull Problem (Quickhull Algorithm)**

```python
def quickhull(points):
    def find_side(p1, p2, p):
        return (p[0] - p1[0]) * (p2[1] - p1[1]) - (p2[0] - p1[0]) * (p[1] - p1[1])

    def distance(p1, p2, p):
        return abs(find_side(p1, p2, p))

    def hull(points, p1, p2, side):
        index = -1
        max_dist = 0
        for i, p in enumerate(points):
            temp = distance(p1, p2, p)
            if find_side(p1, p2, p) == side and temp > max_dist:
                index = i
                max_dist = temp
        if index == -1:
            hull_points.add((p1, p2))
            return
        hull(points[:index] + points[index + 1:], points[index], p1, -find_side(points[index], p1, p2))
        hull(points[:index] + points[index + 1:], points[index], p2, -find_side(points[index], p2, p1))

    if len(points) <= 3:
        return points

    min_x_point = min(points, key=lambda p: p[0])
    max_x_point = max(points, key=lambda p: p[0])

    hull_points = set()
    hull(points, min_x_point, max_x_point, 1)
    hull(points, min_x_point, max_x_point, -1)

    convex_hull = set()
    for p1, p2 in hull_points:
        convex_hull.add(p1)
        convex_hull.add(p2)

    return list(convex_hull)

# Example usage:
points = [(0, 3), (2, 2), (1, 1), (2, 1), (3, 0), (0, 0), (3, 3)]
convex_hull_points = quickhull(points)
print("Convex Hull points:")
for point in convex_hull_points:
    print(point)
```

**Explanation:**

- **Divide**: Find points that form the extreme ends (leftmost and rightmost).
- **Conquer**: Recursively find the points that form the convex hull on each side.
- **Combine**: Combine the upper and lower hull points to get the convex hull.

---

### 6. **Matrix Multiplication (Strassen's Algorithm)**

(Please note that this algorithm was also provided in the previous section, but here it is again for completeness.)

```python
import numpy as np

def strassen(A, B):
    assert A.shape == B.shape
    n = A.shape[0]
    if n == 1:
        return A * B
    else:
        # Divide matrices into quadrants
        mid = n // 2
        A11 = A[:mid, :mid]
        A12 = A[:mid, mid:]
        A21 = A[mid:, :mid]
        A22 = A[mid:, mid:]

        B11 = B[:mid, :mid]
        B12 = B[:mid, mid:]
        B21 = B[mid:, :mid]
        B22 = B[mid:, mid:]

        # Compute the 7 products
        M1 = strassen(A11 + A22, B11 + B22)
        M2 = strassen(A21 + A22, B11)
        M3 = strassen(A11, B12 - B22)
        M4 = strassen(A22, B21 - B11)
        M5 = strassen(A11 + A12, B22)
        M6 = strassen(A21 - A11, B11 + B12)
        M7 = strassen(A12 - A22, B21 + B22)

        # Compute the result quadrants
        C11 = M1 + M4 - M5 + M7
        C12 = M3 + M5
        C21 = M2 + M4
        C22 = M1 - M2 + M3 + M6

        # Combine quadrants into a full matrix
        C = np.zeros((n, n), dtype=A.dtype)
        C[:mid, :mid] = C11
        C[:mid, mid:] = C12
        C[mid:, :mid] = C21
        C[mid:, mid:] = C22
        return C

# Example usage:
A = np.array([[1, 3], [7, 5]])
B = np.array([[6, 8], [4, 2]])
C = strassen(A, B)
print("Product matrix:\n", C)
```

**Explanation:**

- **Divide**: Split matrices into smaller submatrices (quadrants).
- **Conquer**: Recursively compute the 7 necessary multiplications.
- **Combine**: Use the Strassen formula to combine the submatrices into the final product.

---

**Note:** For the Karatsuba algorithm and the Closest Pair of Points, it's essential to ensure that the numbers or the number of points are sufficiently large to see the efficiency gains over traditional methods.




















---
---
---




### Transform and Conquer Overview
- **Transform and Conquer** involves transforming a problem to make it easier to solve. There are three main types of transformations:
  1. **Instance Simplification**: Transform the problem into a more convenient instance of the same problem.
  2. **Representation Change**: Change the way the problem or data is represented.
  3. **Problem Reduction**: Transform the problem into a different problem that is easier to solve or already has a known solution.

### Key Techniques in Transform and Conquer

#### 1. Instance Simplification
- **Presorting**:
  - Many problems are easier to solve once the input is sorted.
  - Common examples include:
    - **Median Selection**: Finding the median or other statistics becomes easier with presorting.
    - **Finding Repeated Elements**: Identifying duplicates is simpler in a sorted list.
    - **Closest Pair & Convex Hull Problems**: Both geometric problems can benefit from sorting to reduce their complexity.
  - **Efficiency**:
    - Presorting takes \( O(n \log n) \) time, but it can improve the efficiency of certain problems from \( O(n^2) \) to \( O(n \log n) \).
  - [lec9c.pdf, pages 21-23](32)

- **Gaussian Elimination**:
  - A technique used to simplify systems of linear equations.
  - **Steps**:
    - Transform the system into an upper triangular form.
    - Use **backward substitution** to solve the simplified system.
  - **Time Complexity**: \( O(n^3) \).
  - [lec9c.pdf, page 20](32)

#### 2. Representation Change
- **Balanced Search Trees**:
  - These trees, such as AVL trees or Red-Black Trees, ensure operations (insertions, deletions, lookups) remain efficient by maintaining balance.
  - Operations like searching, inserting, and deleting all have \( O(\log n) \) complexity.
  - **Efficiency**: Balanced search trees provide better performance compared to unbalanced trees.


- **Heaps and Heapsort**:
  - **Heaps** are binary trees that maintain a specific order property. They are used in implementing priority queues.
  - **Heapsort** uses a heap to sort an array in \( O(n \log n) \) time by building a max-heap and repeatedly extracting the maximum element.
  - **Efficiency**: Heapsort is efficient with a time complexity of \( O(n \log n) \), and unlike Quicksort, it has a worst-case performance guarantee.

- **Polynomial Evaluation by Horner’s Rule**:
  - **Horner’s Rule** optimizes polynomial evaluation by reducing the number of operations.
  - Instead of evaluating a polynomial \( p(x) = a_n x^n + a_{n-1} x^{n-1} + \dots + a_1 x + a_0 \) directly (which would require \( O(n^2) \) operations), it restructures it as:
    \[
    p(x) = (((a_n x + a_{n-1})x + a_{n-2}) \dots x + a_1)x + a_0
    \]
  - This reduces the number of multiplications to \( O(n) \).
  - **Efficiency**: This method is much faster than the brute force approach, especially for large polynomials.
  - [lec10.pdf, pages 5-7](33)

- **Binary Exponentiation**:
  - Used to compute powers of a number efficiently by leveraging the binary representation of the exponent.
  - For example, to compute \( a^n \), instead of multiplying \( a \) by itself \( n \) times, the exponent is broken down using its binary representation.
  - **Time Complexity**: Binary exponentiation runs in \( O(\log n) \) time, which is much faster than the brute force \( O(n) \) approach.
  - [lec10.pdf, pages 9-13](33)

#### 3. Problem Reduction
- **Least Common Multiple (LCM)**:
  - The **LCM** of two integers can be found by reducing the problem to the **Greatest Common Divisor (GCD)** problem using the relation:
    \[
    \text{LCM}(m, n) = \frac{m \times n}{\text{GCD}(m, n)}
    \]
  - **Efficiency**: This problem reduction is efficient since GCD can be computed in \( O(\log n) \) using Euclid’s algorithm.
  - [lec10.pdf, page 15](33)

- **Reduction to Graph Problems**:
  - Many problems can be transformed into graph-based problems, where the solution involves finding paths, connected components, or minimal spanning trees.
  - **Example**: The **River Crossing Puzzle** can be modeled as a state-space graph, where vertices represent different configurations of the puzzle (who's on each side of the river) and edges represent valid transitions.
  - This type of problem is solved by finding the shortest path in the state-space graph.
  - **Efficiency**: Solutions to graph problems often depend on algorithms such as BFS (Breadth-First Search) or DFS (Depth-First Search), both of which run in \( O(V + E) \) time.
  - [lec10.pdf, pages 16-17](33)

---

### Strengths and Weaknesses of Transform and Conquer
- **Strengths**:
  - **Powerful data structures**: Transform and conquer allows the application of advanced data structures (like heaps, search trees) to solve problems efficiently.
  - **Optimization**: It can significantly reduce the complexity of problems through instance simplification, representation changes, or problem reduction.
  - **Flexibility**: It’s a flexible technique, applicable to a wide variety of problems, including computational geometry, algebraic problems, and optimization problems.

- **Weaknesses**:
  - **Complexity of derivation**: Coming up with the right transformation or reduction is often challenging.
  - **Preprocessing overhead**: The transformation itself may add overhead (e.g., presorting), which needs to be justified by gains in efficiency during the problem-solving phase.
  - [lec10.pdf, pages 18-19](33)

---

### Examples of Transform and Conquer in Practice
1. **Horner’s Rule for Polynomial Evaluation**:
   - Problem: Efficiently evaluating a polynomial \( p(x) \) for a given \( x \).
   - Solution: Use Horner’s Rule to reduce the number of multiplications.
   - **Time Complexity**: \( O(n) \), where \( n \) is the degree of the polynomial.

2. **Binary Exponentiation**:
   - Problem: Compute powers of a number \( a^n \) for large \( n \).
   - Solution: Use the binary representation of \( n \) to reduce the number of multiplications.
   - **Time Complexity**: \( O(\log n) \).

3. **Reduction of LCM to GCD**:
   - Problem: Find the least common multiple of two numbers.
   - Solution: Use the relation between LCM and GCD.
   - **Time Complexity**: \( O(\log n) \).

---

